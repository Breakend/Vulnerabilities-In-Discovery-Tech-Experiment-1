Add support for using a non-zero "init" value for shadow PTEs, which is<br>
required to enable EPT Violation #VEs in hardware.  When #VEs are<br>
enabled, KVM needs to set the "suppress #VE" bit in unused PTEs to avoid<br>
unintentionally reflecting not-present EPT Violations into the guest.<br>
<br>
Signed-off-by: Sean Christopherson <sean.j.christopherson@xxxxxxxxx><br>
---<br>
 arch/x86/kvm/mmu.h             |  1 +<br>
 arch/x86/kvm/mmu/mmu.c         | 43 ++++++++++++++++++++++++++++------<br>
 arch/x86/kvm/mmu/paging_tmpl.h |  2 +-<br>
 3 files changed, 38 insertions(+), 8 deletions(-)<br>
<br>
diff --git a/arch/x86/kvm/mmu.h b/arch/x86/kvm/mmu.h<br>
index 8a3b1bce722a..139db8a125d6 100644<br>
--- a/arch/x86/kvm/mmu.h<br>
+++ b/arch/x86/kvm/mmu.h<br>
@@ -52,6 +52,7 @@ static inline u64 rsvd_bits(int s, int e)<br>
 }<br>
 <br>
 void kvm_mmu_set_mmio_spte_mask(u64 mmio_mask, u64 mmio_value, u64 access_mask);<br>
+void kvm_mmu_set_spte_init_value(u64 init_value);<br>
 <br>
 void<br>
 reset_shadow_zero_bits_mask(struct kvm_vcpu *vcpu, struct kvm_mmu *context);<br>
diff --git a/arch/x86/kvm/mmu/mmu.c b/arch/x86/kvm/mmu/mmu.c<br>
index 8071952e9cf2..742ea9c254c4 100644<br>
--- a/arch/x86/kvm/mmu/mmu.c<br>
+++ b/arch/x86/kvm/mmu/mmu.c<br>
@@ -250,6 +250,8 @@ static u64 __read_mostly shadow_mmio_access_mask;<br>
 static u64 __read_mostly shadow_present_mask;<br>
 static u64 __read_mostly shadow_me_mask;<br>
 <br>
+static u64 __read_mostly shadow_init_value;<br>
+<br>
 /*<br>
  * SPTEs used by MMUs without A/D bits are marked with SPTE_AD_DISABLED_MASK;<br>
  * shadow_acc_track_mask is the set of bits to be cleared in non-accessed<br>
@@ -538,6 +540,13 @@ void kvm_mmu_set_mask_ptes(u64 user_mask, u64 accessed_mask,<br>
 }<br>
 EXPORT_SYMBOL_GPL(kvm_mmu_set_mask_ptes);<br>
 <br>
+void kvm_mmu_set_spte_init_value(u64 init_value)<br>
+{<br>
+	WARN_ON(!IS_ENABLED(CONFIG_X86_64) && init_value);<br>
+	shadow_init_value = init_value;<br>
+}<br>
+EXPORT_SYMBOL_GPL(kvm_mmu_set_spte_init_value);<br>
+<br>
 static u8 kvm_get_shadow_phys_bits(void)<br>
 {<br>
 	/*<br>
@@ -569,6 +578,7 @@ static void kvm_mmu_reset_all_pte_masks(void)<br>
 	shadow_mmio_mask = 0;<br>
 	shadow_present_mask = 0;<br>
 	shadow_acc_track_mask = 0;<br>
+	shadow_init_value = 0;<br>
 <br>
 	shadow_phys_bits = kvm_get_shadow_phys_bits();<br>
 <br>
@@ -610,7 +620,7 @@ static int is_nx(struct kvm_vcpu *vcpu)<br>
 <br>
 static int is_shadow_present_pte(u64 pte)<br>
 {<br>
-	return (pte != 0) && !is_mmio_spte(pte);<br>
+	return (pte != 0 && pte != shadow_init_value && !is_mmio_spte(pte));<br>
 }<br>
 <br>
 static int is_large_pte(u64 pte)<br>
@@ -921,9 +931,9 @@ static int mmu_spte_clear_track_bits(u64 *sptep)<br>
 	u64 old_spte = *sptep;<br>
 <br>
 	if (!spte_has_volatile_bits(old_spte))<br>
-		__update_clear_spte_fast(sptep, 0ull);<br>
+		__update_clear_spte_fast(sptep, shadow_init_value);<br>
 	else<br>
-		old_spte = __update_clear_spte_slow(sptep, 0ull);<br>
+		old_spte = __update_clear_spte_slow(sptep, shadow_init_value);<br>
 <br>
 	if (!is_shadow_present_pte(old_spte))<br>
 		return 0;<br>
@@ -953,7 +963,7 @@ static int mmu_spte_clear_track_bits(u64 *sptep)<br>
  */<br>
 static void mmu_spte_clear_no_track(u64 *sptep)<br>
 {<br>
-	__update_clear_spte_fast(sptep, 0ull);<br>
+	__update_clear_spte_fast(sptep, shadow_init_value);<br>
 }<br>
 <br>
 static u64 mmu_spte_get_lockless(u64 *sptep)<br>
@@ -2473,6 +2483,20 @@ static void clear_sp_write_flooding_count(u64 *spte)<br>
 	__clear_sp_write_flooding_count(sp);<br>
 }<br>
 <br>
+#ifdef CONFIG_X86_64<br>
+static inline void kvm_clear_ptes(void *page)<br>
+{<br>
+	int ign;<br>
+<br>
+	asm volatile (<br>
+		"rep stosq\n\t"<br>
+		: "=c"(ign), "=D"(page)<br>
+		: "a"(shadow_init_value), "c"(4096/8), "D"(page)<br>
+		: "memory"<br>
+	);<br>
+}<br>
+#endif<br>
+<br>
 static struct kvm_mmu_page *kvm_mmu_get_page(struct kvm_vcpu *vcpu,<br>
 					     gfn_t gfn,<br>
 					     gva_t gaddr,<br>
@@ -2553,7 +2577,12 @@ static struct kvm_mmu_page *kvm_mmu_get_page(struct kvm_vcpu *vcpu,<br>
 		if (level > PT_PAGE_TABLE_LEVEL && need_sync)<br>
 			flush |= kvm_sync_pages(vcpu, gfn, &invalid_list);<br>
 	}<br>
-	clear_page(sp->spt);<br>
+#ifdef CONFIG_X86_64<br>
+	if (shadow_init_value)<br>
+		kvm_clear_ptes(sp->spt);<br>
+	else<br>
+#endif<br>
+		clear_page(sp->spt);<br>
 	trace_kvm_mmu_get_page(sp, true);<br>
 <br>
 	kvm_mmu_flush_or_zap(vcpu, &invalid_list, false, flush);<br>
@@ -3515,7 +3544,7 @@ static bool fast_page_fault(struct kvm_vcpu *vcpu, gpa_t cr2_or_gpa,<br>
 	struct kvm_shadow_walk_iterator iterator;<br>
 	struct kvm_mmu_page *sp;<br>
 	bool fault_handled = false;<br>
-	u64 spte = 0ull;<br>
+	u64 spte = shadow_init_value;<br>
 	uint retry_count = 0;<br>
 <br>
 	if (!page_fault_can_be_fast(error_code))<br>
@@ -3951,7 +3980,7 @@ static bool<br>
 walk_shadow_page_get_mmio_spte(struct kvm_vcpu *vcpu, u64 addr, u64 *sptep)<br>
 {<br>
 	struct kvm_shadow_walk_iterator iterator;<br>
-	u64 sptes[PT64_ROOT_MAX_LEVEL], spte = 0ull;<br>
+	u64 sptes[PT64_ROOT_MAX_LEVEL], spte = shadow_init_value;<br>
 	struct rsvd_bits_validate *rsvd_check;<br>
 	int root, leaf;<br>
 	bool reserved = false;<br>
diff --git a/arch/x86/kvm/mmu/paging_tmpl.h b/arch/x86/kvm/mmu/paging_tmpl.h<br>
index 9bdf9b7d9a96..949deed15933 100644<br>
--- a/arch/x86/kvm/mmu/paging_tmpl.h<br>
+++ b/arch/x86/kvm/mmu/paging_tmpl.h<br>
@@ -1025,7 +1025,7 @@ static int FNAME(sync_page)(struct kvm_vcpu *vcpu, struct kvm_mmu_page *sp)<br>
 		gpa_t pte_gpa;<br>
 		gfn_t gfn;<br>
 <br>
-		if (!sp->spt[i])<br>
+		if (!sp->spt[i] || sp->spt[i] == shadow_init_value)<br>
 			continue;<br>
 <br>
 		pte_gpa = first_pte_gpa + i * sizeof(pt_element_t);<br>
-- <br>
2.24.1<br>
<br>
<br>
--nFreZHaLTZJo0R7j--<br>
<br>
<br>

