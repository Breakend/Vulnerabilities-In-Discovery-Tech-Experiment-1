Add an interface to the netfs helper library for reading data from the<br>
cache instead of downloading it from the server and support for writing<br>
data just downloaded or cleared to the cache.<br>
<br>
The API passes an iov_iter to the cache read/write routines to indicate the<br>
data/buffer to be used.  This is done using the ITER_XARRAY type to provide<br>
direct access to the netfs inode's pagecache.<br>
<br>
When the netfs's ->begin_cache_operation() method is called, this must fill<br>
in the cache_resources in the netfs_read_request struct, including the<br>
netfs_cache_ops used by the helper lib to talk to the cache.  The helper<br>
lib does not directly access the cache.<br>
<br>
Changes:<br>
v6:<br>
- Call trace_netfs_read() after beginning the cache op so that the cookie<br>
  debug ID can be logged[3].<br>
- Don't record the error from writing to the cache.  We don't want to pass<br>
  it back to the netfs[4].<br>
- Fix copy-to-cache subreq amalgamation to not round up as it goes along<br>
  otherwise it overcalculates the length of the write[5].<br>
<br>
v5:<br>
- Use end_page_fscache() rather than unlock_page_fscache()[2].<br>
<br>
v4:<br>
- Added flag to netfs_subreq_terminated() to indicate that the caller may<br>
  have been running async and stuff that might sleep needs punting to a<br>
  workqueue (can't use in_softirq()[1]).<br>
- Add missing inc of netfs_n_rh_read stat.<br>
- Move initial definition of fscache_begin_read_operation() elsewhere.<br>
- Need to call op->begin_cache_operation() from netfs_write_begin().<br>
<br>
Signed-off-by: David Howells <dhowells@xxxxxxxxxx><br>
Reviewed-by: Jeff Layton <jlayton@xxxxxxxxxx><br>
cc: Matthew Wilcox <willy@xxxxxxxxxxxxx><br>
cc: linux-mm@xxxxxxxxx<br>
cc: linux-cachefs@xxxxxxxxxx<br>
cc: linux-afs@xxxxxxxxxxxxxxxxxxx<br>
cc: linux-nfs@xxxxxxxxxxxxxxx<br>
cc: linux-cifs@xxxxxxxxxxxxxxx<br>
cc: ceph-devel@xxxxxxxxxxxxxxx<br>
cc: v9fs-developer@xxxxxxxxxxxxxxxxxxxxx<br>
cc: linux-fsdevel@xxxxxxxxxxxxxxx<br>
Link: <a  rel="nofollow" href="https://lore.kernel.org/r/20210216084230.GA23669@xxxxxx/">https://lore.kernel.org/r/20210216084230.GA23669@xxxxxx/</a> [1]<br>
Link: <a  rel="nofollow" href="https://lore.kernel.org/r/2499407.1616505440@xxxxxxxxxxxxxxxxxxxxxx/">https://lore.kernel.org/r/2499407.1616505440@xxxxxxxxxxxxxxxxxxxxxx/</a> [2]<br>
Link: <a  rel="nofollow" href="https://lore.kernel.org/r/161781045123.463527.14533348855710902201.stgit@xxxxxxxxxxxxxxxxxxxxxx/">https://lore.kernel.org/r/161781045123.463527.14533348855710902201.stgit@xxxxxxxxxxxxxxxxxxxxxx/</a> [3]<br>
Link: <a  rel="nofollow" href="https://lore.kernel.org/r/161781046256.463527.18158681600085556192.stgit@xxxxxxxxxxxxxxxxxxxxxx/">https://lore.kernel.org/r/161781046256.463527.18158681600085556192.stgit@xxxxxxxxxxxxxxxxxxxxxx/</a> [4]<br>
Link: <a  rel="nofollow" href="https://lore.kernel.org/r/161781047695.463527.7463536103593997492.stgit@xxxxxxxxxxxxxxxxxxxxxx/">https://lore.kernel.org/r/161781047695.463527.7463536103593997492.stgit@xxxxxxxxxxxxxxxxxxxxxx/</a> [5]<br>
Link: <a  rel="nofollow" href="https://lore.kernel.org/r/161118141321.1232039.8296910406755622458.stgit@xxxxxxxxxxxxxxxxxxxxxx/">https://lore.kernel.org/r/161118141321.1232039.8296910406755622458.stgit@xxxxxxxxxxxxxxxxxxxxxx/</a> # rfc<br>
Link: <a  rel="nofollow" href="https://lore.kernel.org/r/161161036700.2537118.11170748455436854978.stgit@xxxxxxxxxxxxxxxxxxxxxx/">https://lore.kernel.org/r/161161036700.2537118.11170748455436854978.stgit@xxxxxxxxxxxxxxxxxxxxxx/</a> # v2<br>
Link: <a  rel="nofollow" href="https://lore.kernel.org/r/161340399569.1303470.1138884774643385730.stgit@xxxxxxxxxxxxxxxxxxxxxx/">https://lore.kernel.org/r/161340399569.1303470.1138884774643385730.stgit@xxxxxxxxxxxxxxxxxxxxxx/</a> # v3<br>
Link: <a  rel="nofollow" href="https://lore.kernel.org/r/161539542874.286939.13337898213448136687.stgit@xxxxxxxxxxxxxxxxxxxxxx/">https://lore.kernel.org/r/161539542874.286939.13337898213448136687.stgit@xxxxxxxxxxxxxxxxxxxxxx/</a> # v4<br>
Link: <a  rel="nofollow" href="https://lore.kernel.org/r/161653799826.2770958.9015430297426331950.stgit@xxxxxxxxxxxxxxxxxxxxxx/">https://lore.kernel.org/r/161653799826.2770958.9015430297426331950.stgit@xxxxxxxxxxxxxxxxxxxxxx/</a> # v5<br>
---<br>
<br>
 fs/netfs/read_helper.c       |  239 ++++++++++++++++++++++++++++++++++++++++++<br>
 include/linux/netfs.h        |   55 ++++++++++<br>
 include/trace/events/netfs.h |    2 <br>
 3 files changed, 295 insertions(+), 1 deletion(-)<br>
<br>
diff --git a/fs/netfs/read_helper.c b/fs/netfs/read_helper.c<br>
index da34aedea053..cd3b61d5e192 100644<br>
--- a/fs/netfs/read_helper.c<br>
+++ b/fs/netfs/read_helper.c<br>
@@ -88,6 +88,8 @@ static void netfs_free_read_request(struct work_struct *work)<br>
 	if (rreq->netfs_priv)<br>
 		rreq->netfs_ops->cleanup(rreq->mapping, rreq->netfs_priv);<br>
 	trace_netfs_rreq(rreq, netfs_rreq_trace_free);<br>
+	if (rreq->cache_resources.ops)<br>
+		rreq->cache_resources.ops->end_operation(&rreq->cache_resources);<br>
 	kfree(rreq);<br>
 	netfs_stat_d(&netfs_n_rh_rreq);<br>
 }<br>
@@ -154,6 +156,34 @@ static void netfs_clear_unread(struct netfs_read_subrequest *subreq)<br>
 	iov_iter_zero(iov_iter_count(&iter), &iter);<br>
 }<br>
 <br>
+static void netfs_cache_read_terminated(void *priv, ssize_t transferred_or_error,<br>
+					bool was_async)<br>
+{<br>
+	struct netfs_read_subrequest *subreq = priv;<br>
+<br>
+	netfs_subreq_terminated(subreq, transferred_or_error, was_async);<br>
+}<br>
+<br>
+/*<br>
+ * Issue a read against the cache.<br>
+ * - Eats the caller's ref on subreq.<br>
+ */<br>
+static void netfs_read_from_cache(struct netfs_read_request *rreq,<br>
+				  struct netfs_read_subrequest *subreq,<br>
+				  bool seek_data)<br>
+{<br>
+	struct netfs_cache_resources *cres = &rreq->cache_resources;<br>
+	struct iov_iter iter;<br>
+<br>
+	netfs_stat(&netfs_n_rh_read);<br>
+	iov_iter_xarray(&iter, READ, &rreq->mapping->i_pages,<br>
+			subreq->start + subreq->transferred,<br>
+			subreq->len   - subreq->transferred);<br>
+<br>
+	cres->ops->read(cres, subreq->start, &iter, seek_data,<br>
+			netfs_cache_read_terminated, subreq);<br>
+}<br>
+<br>
 /*<br>
  * Fill a subrequest region with zeroes.<br>
  */<br>
@@ -198,6 +228,141 @@ static void netfs_rreq_completed(struct netfs_read_request *rreq, bool was_async<br>
 	netfs_put_read_request(rreq, was_async);<br>
 }<br>
 <br>
+/*<br>
+ * Deal with the completion of writing the data to the cache.  We have to clear<br>
+ * the PG_fscache bits on the pages involved and release the caller's ref.<br>
+ *<br>
+ * May be called in softirq mode and we inherit a ref from the caller.<br>
+ */<br>
+static void netfs_rreq_unmark_after_write(struct netfs_read_request *rreq,<br>
+					  bool was_async)<br>
+{<br>
+	struct netfs_read_subrequest *subreq;<br>
+	struct page *page;<br>
+	pgoff_t unlocked = 0;<br>
+	bool have_unlocked = false;<br>
+<br>
+	rcu_read_lock();<br>
+<br>
+	list_for_each_entry(subreq, &rreq->subrequests, rreq_link) {<br>
+		XA_STATE(xas, &rreq->mapping->i_pages, subreq->start / PAGE_SIZE);<br>
+<br>
+		xas_for_each(&xas, page, (subreq->start + subreq->len - 1) / PAGE_SIZE) {<br>
+			/* We might have multiple writes from the same huge<br>
+			 * page, but we mustn't unlock a page more than once.<br>
+			 */<br>
+			if (have_unlocked && page->index <= unlocked)<br>
+				continue;<br>
+			unlocked = page->index;<br>
+			end_page_fscache(page);<br>
+			have_unlocked = true;<br>
+		}<br>
+	}<br>
+<br>
+	rcu_read_unlock();<br>
+	netfs_rreq_completed(rreq, was_async);<br>
+}<br>
+<br>
+static void netfs_rreq_copy_terminated(void *priv, ssize_t transferred_or_error,<br>
+				       bool was_async)<br>
+{<br>
+	struct netfs_read_subrequest *subreq = priv;<br>
+	struct netfs_read_request *rreq = subreq->rreq;<br>
+<br>
+	if (IS_ERR_VALUE(transferred_or_error)) {<br>
+		netfs_stat(&netfs_n_rh_write_failed);<br>
+	} else {<br>
+		netfs_stat(&netfs_n_rh_write_done);<br>
+	}<br>
+<br>
+	trace_netfs_sreq(subreq, netfs_sreq_trace_write_term);<br>
+<br>
+	/* If we decrement nr_wr_ops to 0, the ref belongs to us. */<br>
+	if (atomic_dec_and_test(&rreq->nr_wr_ops))<br>
+		netfs_rreq_unmark_after_write(rreq, was_async);<br>
+<br>
+	netfs_put_subrequest(subreq, was_async);<br>
+}<br>
+<br>
+/*<br>
+ * Perform any outstanding writes to the cache.  We inherit a ref from the<br>
+ * caller.<br>
+ */<br>
+static void netfs_rreq_do_write_to_cache(struct netfs_read_request *rreq)<br>
+{<br>
+	struct netfs_cache_resources *cres = &rreq->cache_resources;<br>
+	struct netfs_read_subrequest *subreq, *next, *p;<br>
+	struct iov_iter iter;<br>
+	int ret;<br>
+<br>
+	trace_netfs_rreq(rreq, netfs_rreq_trace_write);<br>
+<br>
+	/* We don't want terminating writes trying to wake us up whilst we're<br>
+	 * still going through the list.<br>
+	 */<br>
+	atomic_inc(&rreq->nr_wr_ops);<br>
+<br>
+	list_for_each_entry_safe(subreq, p, &rreq->subrequests, rreq_link) {<br>
+		if (!test_bit(NETFS_SREQ_WRITE_TO_CACHE, &subreq->flags)) {<br>
+			list_del_init(&subreq->rreq_link);<br>
+			netfs_put_subrequest(subreq, false);<br>
+		}<br>
+	}<br>
+<br>
+	list_for_each_entry(subreq, &rreq->subrequests, rreq_link) {<br>
+		/* Amalgamate adjacent writes */<br>
+		while (!list_is_last(&subreq->rreq_link, &rreq->subrequests)) {<br>
+			next = list_next_entry(subreq, rreq_link);<br>
+			if (next->start != subreq->start + subreq->len)<br>
+				break;<br>
+			subreq->len += next->len;<br>
+			list_del_init(&next->rreq_link);<br>
+			netfs_put_subrequest(next, false);<br>
+		}<br>
+<br>
+		ret = cres->ops->prepare_write(cres, &subreq->start, &subreq->len,<br>
+					       rreq->i_size);<br>
+		if (ret < 0) {<br>
+			trace_netfs_sreq(subreq, netfs_sreq_trace_write_skip);<br>
+			continue;<br>
+		}<br>
+<br>
+		iov_iter_xarray(&iter, WRITE, &rreq->mapping->i_pages,<br>
+				subreq->start, subreq->len);<br>
+<br>
+		atomic_inc(&rreq->nr_wr_ops);<br>
+		netfs_stat(&netfs_n_rh_write);<br>
+		netfs_get_read_subrequest(subreq);<br>
+		trace_netfs_sreq(subreq, netfs_sreq_trace_write);<br>
+		cres->ops->write(cres, subreq->start, &iter,<br>
+				 netfs_rreq_copy_terminated, subreq);<br>
+	}<br>
+<br>
+	/* If we decrement nr_wr_ops to 0, the usage ref belongs to us. */<br>
+	if (atomic_dec_and_test(&rreq->nr_wr_ops))<br>
+		netfs_rreq_unmark_after_write(rreq, false);<br>
+}<br>
+<br>
+static void netfs_rreq_write_to_cache_work(struct work_struct *work)<br>
+{<br>
+	struct netfs_read_request *rreq =<br>
+		container_of(work, struct netfs_read_request, work);<br>
+<br>
+	netfs_rreq_do_write_to_cache(rreq);<br>
+}<br>
+<br>
+static void netfs_rreq_write_to_cache(struct netfs_read_request *rreq,<br>
+				      bool was_async)<br>
+{<br>
+	if (was_async) {<br>
+		rreq->work.func = netfs_rreq_write_to_cache_work;<br>
+		if (!queue_work(system_unbound_wq, &rreq->work))<br>
+			BUG();<br>
+	} else {<br>
+		netfs_rreq_do_write_to_cache(rreq);<br>
+	}<br>
+}<br>
+<br>
 /*<br>
  * Unlock the pages in a read operation.  We need to set PG_fscache on any<br>
  * pages we're going to write back before we unlock them.<br>
@@ -299,7 +464,10 @@ static void netfs_rreq_short_read(struct netfs_read_request *rreq,<br>
 <br>
 	netfs_get_read_subrequest(subreq);<br>
 	atomic_inc(&rreq->nr_rd_ops);<br>
-	netfs_read_from_server(rreq, subreq);<br>
+	if (subreq->source == NETFS_READ_FROM_CACHE)<br>
+		netfs_read_from_cache(rreq, subreq, true);<br>
+	else<br>
+		netfs_read_from_server(rreq, subreq);<br>
 }<br>
 <br>
 /*<br>
@@ -344,6 +512,25 @@ static bool netfs_rreq_perform_resubmissions(struct netfs_read_request *rreq)<br>
 	return false;<br>
 }<br>
 <br>
+/*<br>
+ * Check to see if the data read is still valid.<br>
+ */<br>
+static void netfs_rreq_is_still_valid(struct netfs_read_request *rreq)<br>
+{<br>
+	struct netfs_read_subrequest *subreq;<br>
+<br>
+	if (!rreq->netfs_ops->is_still_valid ||<br>
+	    rreq->netfs_ops->is_still_valid(rreq))<br>
+		return;<br>
+<br>
+	list_for_each_entry(subreq, &rreq->subrequests, rreq_link) {<br>
+		if (subreq->source == NETFS_READ_FROM_CACHE) {<br>
+			subreq->error = -ESTALE;<br>
+			__set_bit(NETFS_RREQ_INCOMPLETE_IO, &rreq->flags);<br>
+		}<br>
+	}<br>
+}<br>
+<br>
 /*<br>
  * Assess the state of a read request and decide what to do next.<br>
  *<br>
@@ -355,6 +542,8 @@ static void netfs_rreq_assess(struct netfs_read_request *rreq, bool was_async)<br>
 	trace_netfs_rreq(rreq, netfs_rreq_trace_assess);<br>
 <br>
 again:<br>
+	netfs_rreq_is_still_valid(rreq);<br>
+<br>
 	if (!test_bit(NETFS_RREQ_FAILED, &rreq->flags) &&<br>
 	    test_bit(NETFS_RREQ_INCOMPLETE_IO, &rreq->flags)) {<br>
 		if (netfs_rreq_perform_resubmissions(rreq))<br>
@@ -367,6 +556,9 @@ static void netfs_rreq_assess(struct netfs_read_request *rreq, bool was_async)<br>
 	clear_bit_unlock(NETFS_RREQ_IN_PROGRESS, &rreq->flags);<br>
 	wake_up_bit(&rreq->flags, NETFS_RREQ_IN_PROGRESS);<br>
 <br>
+	if (test_bit(NETFS_RREQ_WRITE_TO_CACHE, &rreq->flags))<br>
+		return netfs_rreq_write_to_cache(rreq, was_async);<br>
+<br>
 	netfs_rreq_completed(rreq, was_async);<br>
 }<br>
 <br>
@@ -504,7 +696,10 @@ static enum netfs_read_source netfs_cache_prepare_read(struct netfs_read_subrequ<br>
 						       loff_t i_size)<br>
 {<br>
 	struct netfs_read_request *rreq = subreq->rreq;<br>
+	struct netfs_cache_resources *cres = &rreq->cache_resources;<br>
 <br>
+	if (cres->ops)<br>
+		return cres->ops->prepare_read(subreq, i_size);<br>
 	if (subreq->start >= rreq->i_size)<br>
 		return NETFS_FILL_WITH_ZEROES;<br>
 	return NETFS_DOWNLOAD_FROM_SERVER;<br>
@@ -595,6 +790,9 @@ static bool netfs_rreq_submit_slice(struct netfs_read_request *rreq,<br>
 	case NETFS_DOWNLOAD_FROM_SERVER:<br>
 		netfs_read_from_server(rreq, subreq);<br>
 		break;<br>
+	case NETFS_READ_FROM_CACHE:<br>
+		netfs_read_from_cache(rreq, subreq, false);<br>
+		break;<br>
 	default:<br>
 		BUG();<br>
 	}<br>
@@ -607,9 +805,23 @@ static bool netfs_rreq_submit_slice(struct netfs_read_request *rreq,<br>
 	return false;<br>
 }<br>
 <br>
+static void netfs_cache_expand_readahead(struct netfs_read_request *rreq,<br>
+					 loff_t *_start, size_t *_len, loff_t i_size)<br>
+{<br>
+	struct netfs_cache_resources *cres = &rreq->cache_resources;<br>
+<br>
+	if (cres->ops && cres->ops->expand_readahead)<br>
+		cres->ops->expand_readahead(cres, _start, _len, i_size);<br>
+}<br>
+<br>
 static void netfs_rreq_expand(struct netfs_read_request *rreq,<br>
 			      struct readahead_control *ractl)<br>
 {<br>
+	/* Give the cache a chance to change the request parameters.  The<br>
+	 * resultant request must contain the original region.<br>
+	 */<br>
+	netfs_cache_expand_readahead(rreq, &rreq->start, &rreq->len, rreq->i_size);<br>
+<br>
 	/* Give the netfs a chance to change the request parameters.  The<br>
 	 * resultant request must contain the original region.<br>
 	 */<br>
@@ -661,6 +873,7 @@ void netfs_readahead(struct readahead_control *ractl,<br>
 	struct netfs_read_request *rreq;<br>
 	struct page *page;<br>
 	unsigned int debug_index = 0;<br>
+	int ret;<br>
 <br>
 	_enter("%lx,%x", readahead_index(ractl), readahead_count(ractl));<br>
 <br>
@@ -674,6 +887,12 @@ void netfs_readahead(struct readahead_control *ractl,<br>
 	rreq->start	= readahead_pos(ractl);<br>
 	rreq->len	= readahead_length(ractl);<br>
 <br>
+	if (ops->begin_cache_operation) {<br>
+		ret = ops->begin_cache_operation(rreq);<br>
+		if (ret == -ENOMEM || ret == -EINTR || ret == -ERESTARTSYS)<br>
+			goto cleanup_free;<br>
+	}<br>
+<br>
 	netfs_stat(&netfs_n_rh_readahead);<br>
 	trace_netfs_read(rreq, readahead_pos(ractl), readahead_length(ractl),<br>
 			 netfs_read_trace_readahead);<br>
@@ -698,6 +917,9 @@ void netfs_readahead(struct readahead_control *ractl,<br>
 		netfs_rreq_assess(rreq, false);<br>
 	return;<br>
 <br>
+cleanup_free:<br>
+	netfs_put_read_request(rreq, false);<br>
+	return;<br>
 cleanup:<br>
 	if (netfs_priv)<br>
 		ops->cleanup(ractl->mapping, netfs_priv);<br>
@@ -744,6 +966,14 @@ int netfs_readpage(struct file *file,<br>
 	rreq->start	= page_index(page) * PAGE_SIZE;<br>
 	rreq->len	= thp_size(page);<br>
 <br>
+	if (ops->begin_cache_operation) {<br>
+		ret = ops->begin_cache_operation(rreq);<br>
+		if (ret == -ENOMEM || ret == -EINTR || ret == -ERESTARTSYS) {<br>
+			unlock_page(page);<br>
+			goto out;<br>
+		}<br>
+	}<br>
+<br>
 	netfs_stat(&netfs_n_rh_readpage);<br>
 	trace_netfs_read(rreq, rreq->start, rreq->len, netfs_read_trace_readpage);<br>
 <br>
@@ -768,6 +998,7 @@ int netfs_readpage(struct file *file,<br>
 	ret = rreq->error;<br>
 	if (ret == 0 && rreq->submitted < rreq->len)<br>
 		ret = -EIO;<br>
+out:<br>
 	netfs_put_read_request(rreq, false);<br>
 	return ret;<br>
 }<br>
@@ -873,6 +1104,12 @@ int netfs_write_begin(struct file *file, struct address_space *mapping,<br>
 	__set_bit(NETFS_RREQ_NO_UNLOCK_PAGE, &rreq->flags);<br>
 	netfs_priv = NULL;<br>
 <br>
+	if (ops->begin_cache_operation) {<br>
+		ret = ops->begin_cache_operation(rreq);<br>
+		if (ret == -ENOMEM || ret == -EINTR || ret == -ERESTARTSYS)<br>
+			goto error_put;<br>
+	}<br>
+<br>
 	netfs_stat(&netfs_n_rh_write_begin);<br>
 	trace_netfs_read(rreq, pos, len, netfs_read_trace_write_begin);<br>
 <br>
diff --git a/include/linux/netfs.h b/include/linux/netfs.h<br>
index 99659ed9524e..9062adfa2fb9 100644<br>
--- a/include/linux/netfs.h<br>
+++ b/include/linux/netfs.h<br>
@@ -92,6 +92,18 @@ enum netfs_read_source {<br>
 	NETFS_INVALID_READ,<br>
 } __mode(byte);<br>
 <br>
+typedef void (*netfs_io_terminated_t)(void *priv, ssize_t transferred_or_error,<br>
+				      bool was_async);<br>
+<br>
+/*<br>
+ * Resources required to do operations on a cache.<br>
+ */<br>
+struct netfs_cache_resources {<br>
+	const struct netfs_cache_ops	*ops;<br>
+	void				*cache_priv;<br>
+	void				*cache_priv2;<br>
+};<br>
+<br>
 /*<br>
  * Descriptor for a single component subrequest.<br>
  */<br>
@@ -121,11 +133,13 @@ struct netfs_read_request {<br>
 	struct work_struct	work;<br>
 	struct inode		*inode;		/* The file being accessed */<br>
 	struct address_space	*mapping;	/* The mapping being accessed */<br>
+	struct netfs_cache_resources cache_resources;<br>
 	struct list_head	subrequests;	/* Requests to fetch I/O from disk or net */<br>
 	void			*netfs_priv;	/* Private data for the netfs */<br>
 	unsigned int		debug_id;<br>
 	unsigned int		cookie_debug_id;<br>
 	atomic_t		nr_rd_ops;	/* Number of read ops in progress */<br>
+	atomic_t		nr_wr_ops;	/* Number of write ops in progress */<br>
 	size_t			submitted;	/* Amount submitted for I/O so far */<br>
 	size_t			len;		/* Length of the request */<br>
 	short			error;		/* 0 or error that occurred */<br>
@@ -149,6 +163,7 @@ struct netfs_read_request {<br>
 struct netfs_read_request_ops {<br>
 	bool (*is_cache_enabled)(struct inode *inode);<br>
 	void (*init_rreq)(struct netfs_read_request *rreq, struct file *file);<br>
+	int (*begin_cache_operation)(struct netfs_read_request *rreq);<br>
 	void (*expand_readahead)(struct netfs_read_request *rreq);<br>
 	bool (*clamp_length)(struct netfs_read_subrequest *subreq);<br>
 	void (*issue_op)(struct netfs_read_subrequest *subreq);<br>
@@ -159,6 +174,46 @@ struct netfs_read_request_ops {<br>
 	void (*cleanup)(struct address_space *mapping, void *netfs_priv);<br>
 };<br>
 <br>
+/*<br>
+ * Table of operations for access to a cache.  This is obtained by<br>
+ * rreq->ops->begin_cache_operation().<br>
+ */<br>
+struct netfs_cache_ops {<br>
+	/* End an operation */<br>
+	void (*end_operation)(struct netfs_cache_resources *cres);<br>
+<br>
+	/* Read data from the cache */<br>
+	int (*read)(struct netfs_cache_resources *cres,<br>
+		    loff_t start_pos,<br>
+		    struct iov_iter *iter,<br>
+		    bool seek_data,<br>
+		    netfs_io_terminated_t term_func,<br>
+		    void *term_func_priv);<br>
+<br>
+	/* Write data to the cache */<br>
+	int (*write)(struct netfs_cache_resources *cres,<br>
+		     loff_t start_pos,<br>
+		     struct iov_iter *iter,<br>
+		     netfs_io_terminated_t term_func,<br>
+		     void *term_func_priv);<br>
+<br>
+	/* Expand readahead request */<br>
+	void (*expand_readahead)(struct netfs_cache_resources *cres,<br>
+				 loff_t *_start, size_t *_len, loff_t i_size);<br>
+<br>
+	/* Prepare a read operation, shortening it to a cached/uncached<br>
+	 * boundary as appropriate.<br>
+	 */<br>
+	enum netfs_read_source (*prepare_read)(struct netfs_read_subrequest *subreq,<br>
+					       loff_t i_size);<br>
+<br>
+	/* Prepare a write operation, working out what part of the write we can<br>
+	 * actually do.<br>
+	 */<br>
+	int (*prepare_write)(struct netfs_cache_resources *cres,<br>
+			     loff_t *_start, size_t *_len, loff_t i_size);<br>
+};<br>
+<br>
 struct readahead_control;<br>
 extern void netfs_readahead(struct readahead_control *,<br>
 			    const struct netfs_read_request_ops *,<br>
diff --git a/include/trace/events/netfs.h b/include/trace/events/netfs.h<br>
index a2bf6cd84bd4..e3ebeabd3852 100644<br>
--- a/include/trace/events/netfs.h<br>
+++ b/include/trace/events/netfs.h<br>
@@ -43,6 +43,7 @@ enum netfs_sreq_trace {<br>
 	netfs_sreq_trace_submit,<br>
 	netfs_sreq_trace_terminated,<br>
 	netfs_sreq_trace_write,<br>
+	netfs_sreq_trace_write_skip,<br>
 	netfs_sreq_trace_write_term,<br>
 };<br>
 <br>
@@ -77,6 +78,7 @@ enum netfs_sreq_trace {<br>
 	EM(netfs_sreq_trace_submit,		"SUBMT")	\<br>
 	EM(netfs_sreq_trace_terminated,		"TERM ")	\<br>
 	EM(netfs_sreq_trace_write,		"WRITE")	\<br>
+	EM(netfs_sreq_trace_write_skip,		"SKIP ")	\<br>
 	E_(netfs_sreq_trace_write_term,		"WTERM")<br>
 <br>
 <br>
<br>
<br>
<br>

