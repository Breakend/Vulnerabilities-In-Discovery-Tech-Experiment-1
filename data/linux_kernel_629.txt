We replace the existing entry to the newly allocated one in case of CoW.<br>
Also, we mark the entry as PAGECACHE_TAG_TOWRITE so writeback marks this<br>
entry as writeprotected.  This helps us snapshots so new write<br>
pagefaults after snapshots trigger a CoW.<br>
<br>
Signed-off-by: Goldwyn Rodrigues <rgoldwyn@xxxxxxxx><br>
Signed-off-by: Shiyang Ruan <ruansy.fnst@xxxxxxxxxxx><br>
Reviewed-by: Christoph Hellwig <hch@xxxxxx><br>
Reviewed-by: Ritesh Harjani <riteshh@xxxxxxxxxxxxx><br>
---<br>
 fs/dax.c | 39 ++++++++++++++++++++++++++++-----------<br>
 1 file changed, 28 insertions(+), 11 deletions(-)<br>
<br>
diff --git a/fs/dax.c b/fs/dax.c<br>
index b4fd3813457a..e6c1354b27a8 100644<br>
--- a/fs/dax.c<br>
+++ b/fs/dax.c<br>
@@ -722,6 +722,10 @@ static int copy_cow_page_dax(struct block_device *bdev, struct dax_device *dax_d<br>
 	return 0;<br>
 }<br>
 <br>
+/* DAX Insert Flag for the entry we insert */<br>
+#define DAX_IF_DIRTY		(1 << 0)<br>
+#define DAX_IF_COW		(1 << 1)<br>
+<br>
 /*<br>
  * By this point grab_mapping_entry() has ensured that we have a locked entry<br>
  * of the appropriate size so we don't have to worry about downgrading PMDs to<br>
@@ -729,16 +733,19 @@ static int copy_cow_page_dax(struct block_device *bdev, struct dax_device *dax_d<br>
  * already in the tree, we will skip the insertion and just dirty the PMD as<br>
  * appropriate.<br>
  */<br>
-static void *dax_insert_entry(struct xa_state *xas,<br>
-		struct address_space *mapping, struct vm_fault *vmf,<br>
-		void *entry, pfn_t pfn, unsigned long flags, bool dirty)<br>
+static void *dax_insert_entry(struct xa_state *xas, struct vm_fault *vmf,<br>
+		void *entry, pfn_t pfn, unsigned long flags,<br>
+		unsigned int insert_flags)<br>
 {<br>
+	struct address_space *mapping = vmf->vma->vm_file->f_mapping;<br>
 	void *new_entry = dax_make_entry(pfn, flags);<br>
+	bool dirty = insert_flags & DAX_IF_DIRTY;<br>
+	bool cow = insert_flags & DAX_IF_COW;<br>
 <br>
 	if (dirty)<br>
 		__mark_inode_dirty(mapping->host, I_DIRTY_PAGES);<br>
 <br>
-	if (dax_is_zero_entry(entry) && !(flags & DAX_ZERO_PAGE)) {<br>
+	if (cow || (dax_is_zero_entry(entry) && !(flags & DAX_ZERO_PAGE))) {<br>
 		unsigned long index = xas->xa_index;<br>
 		/* we are replacing a zero page with block mapping */<br>
 		if (dax_is_pmd_entry(entry))<br>
@@ -750,7 +757,7 @@ static void *dax_insert_entry(struct xa_state *xas,<br>
 <br>
 	xas_reset(xas);<br>
 	xas_lock_irq(xas);<br>
-	if (dax_is_zero_entry(entry) || dax_is_empty_entry(entry)) {<br>
+	if (cow || dax_is_zero_entry(entry) || dax_is_empty_entry(entry)) {<br>
 		void *old;<br>
 <br>
 		dax_disassociate_entry(entry, mapping, false);<br>
@@ -774,6 +781,9 @@ static void *dax_insert_entry(struct xa_state *xas,<br>
 	if (dirty)<br>
 		xas_set_mark(xas, PAGECACHE_TAG_DIRTY);<br>
 <br>
+	if (cow)<br>
+		xas_set_mark(xas, PAGECACHE_TAG_TOWRITE);<br>
+<br>
 	xas_unlock_irq(xas);<br>
 	return entry;<br>
 }<br>
@@ -1109,8 +1119,7 @@ static vm_fault_t dax_load_hole(struct xa_state *xas,<br>
 	pfn_t pfn = pfn_to_pfn_t(my_zero_pfn(vaddr));<br>
 	vm_fault_t ret;<br>
 <br>
-	*entry = dax_insert_entry(xas, mapping, vmf, *entry, pfn,<br>
-			DAX_ZERO_PAGE, false);<br>
+	*entry = dax_insert_entry(xas, vmf, *entry, pfn, DAX_ZERO_PAGE, 0);<br>
 <br>
 	ret = vmf_insert_mixed(vmf->vma, vaddr, pfn);<br>
 	trace_dax_load_hole(inode, vmf, ret);<br>
@@ -1137,8 +1146,8 @@ static vm_fault_t dax_pmd_load_hole(struct xa_state *xas, struct vm_fault *vmf,<br>
 		goto fallback;<br>
 <br>
 	pfn = page_to_pfn_t(zero_page);<br>
-	*entry = dax_insert_entry(xas, mapping, vmf, *entry, pfn,<br>
-			DAX_PMD | DAX_ZERO_PAGE, false);<br>
+	*entry = dax_insert_entry(xas, vmf, *entry, pfn,<br>
+				  DAX_PMD | DAX_ZERO_PAGE, 0);<br>
 <br>
 	if (arch_needs_pgtable_deposit()) {<br>
 		pgtable = pte_alloc_one(vma->vm_mm);<br>
@@ -1444,6 +1453,7 @@ static vm_fault_t dax_fault_actor(struct vm_fault *vmf, pfn_t *pfnp,<br>
 	bool write = vmf->flags & FAULT_FLAG_WRITE;<br>
 	bool sync = dax_fault_is_synchronous(flags, vmf->vma, iomap);<br>
 	unsigned long entry_flags = pmd ? DAX_PMD : 0;<br>
+	unsigned int insert_flags = 0;<br>
 	int err = 0;<br>
 	pfn_t pfn;<br>
 	void *kaddr;<br>
@@ -1466,8 +1476,15 @@ static vm_fault_t dax_fault_actor(struct vm_fault *vmf, pfn_t *pfnp,<br>
 	if (err)<br>
 		return pmd ? VM_FAULT_FALLBACK : dax_fault_return(err);<br>
 <br>
-	*entry = dax_insert_entry(xas, mapping, vmf, *entry, pfn, entry_flags,<br>
-				  write && !sync);<br>
+	if (write) {<br>
+		if (!sync)<br>
+			insert_flags |= DAX_IF_DIRTY;<br>
+		if (iomap->flags & IOMAP_F_SHARED)<br>
+			insert_flags |= DAX_IF_COW;<br>
+	}<br>
+<br>
+	*entry = dax_insert_entry(xas, vmf, *entry, pfn, entry_flags,<br>
+				  insert_flags);<br>
 <br>
 	if (write &&<br>
 	    srcmap->addr != IOMAP_HOLE && srcmap->addr != iomap->addr) {<br>
-- <br>
2.31.0<br>
<br>
<br>
<br>
<br>

