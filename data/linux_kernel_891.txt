
Hi Alex,<br>
<br>
On 2021/4/7 23:31, Alexandru Elisei wrote:<br>
<blockquote style="border-left: #5555EE solid 0.2em; margin: 0em; padding-left: 0.85em">
Hi Yanan,<br>
<br>
On 3/26/21 3:16 AM, Yanan Wang wrote:<br>
<blockquote style="border-left: #5555EE solid 0.2em; margin: 0em; padding-left: 0.85em">
We currently uniformly permorm CMOs of D-cache and I-cache in function<br>
user_mem_abort before calling the fault handlers. If we get concurrent<br>
guest faults(e.g. translation faults, permission faults) or some really<br>
unnecessary guest faults caused by BBM, CMOs for the first vcpu are<br>
</blockquote>
I can't figure out what BBM means.<br>
</blockquote>
Just as Will has explained, it's Break-Before-Make rule. When we need to<br>
replace an old table entry with a new one, we should firstly invalidate<br>
the old table entry(Break), before installation of the new entry(Make).<br>
<br>
And I think this patch mainly introduces benefits in two specific scenarios:<br>
1) In a VM startup, it will improve efficiency of handling page faults 
incurred
<br>
by vCPUs, when initially populating stage2 page tables.<br>
2) After live migration, the heavy workload will be resumed on the 
destination
<br>
VMs, however all the stage2 page tables need to be rebuilt.<br>
<blockquote style="border-left: #5555EE solid 0.2em; margin: 0em; padding-left: 0.85em"><blockquote style="border-left: #5555EE solid 0.2em; margin: 0em; padding-left: 0.85em">
necessary while the others later are not.<br>
<br>
By moving CMOs to the fault handlers, we can easily identify conditions<br>
where they are really needed and avoid the unnecessary ones. As it's a<br>
time consuming process to perform CMOs especially when flushing a block<br>
range, so this solution reduces much load of kvm and improve efficiency<br>
of the page table code.<br>
<br>
So let's move both clean of D-cache and invalidation of I-cache to the<br>
map path and move only invalidation of I-cache to the permission path.<br>
Since the original APIs for CMOs in mmu.c are only called in function<br>
user_mem_abort, we now also move them to pgtable.c.<br>
<br>
Signed-off-by: Yanan Wang <wangyanan55@xxxxxxxxxx><br>
---<br>
  arch/arm64/include/asm/kvm_mmu.h | 31 ---------------<br>
  arch/arm64/kvm/hyp/pgtable.c     | 68 +++++++++++++++++++++++++-------<br>
  arch/arm64/kvm/mmu.c             | 23 ++---------<br>
  3 files changed, 57 insertions(+), 65 deletions(-)<br>
<br>
diff --git a/arch/arm64/include/asm/kvm_mmu.h b/arch/arm64/include/asm/kvm_mmu.h<br>
index 90873851f677..c31f88306d4e 100644<br>
--- a/arch/arm64/include/asm/kvm_mmu.h<br>
+++ b/arch/arm64/include/asm/kvm_mmu.h<br>
@@ -177,37 +177,6 @@ static inline bool vcpu_has_cache_enabled(struct kvm_vcpu *vcpu)<br>
  	return (vcpu_read_sys_reg(vcpu, SCTLR_EL1) & 0b101) == 0b101;<br>
  }<br>
  
-static inline void __clean_dcache_guest_page(kvm_pfn_t pfn, unsigned long size)
<br>
-{<br>
-	void *va = page_address(pfn_to_page(pfn));<br>
-<br>
-	/*<br>
-	 * With FWB, we ensure that the guest always accesses memory using<br>
-	 * cacheable attributes, and we don't have to clean to PoC when<br>
-	 * faulting in pages. Furthermore, FWB implies IDC, so cleaning to<br>
-	 * PoU is not required either in this case.<br>
-	 */<br>
-	if (cpus_have_const_cap(ARM64_HAS_STAGE2_FWB))<br>
-		return;<br>
-<br>
-	kvm_flush_dcache_to_poc(va, size);<br>
-}<br>
-<br>
-static inline void __invalidate_icache_guest_page(kvm_pfn_t pfn,<br>
-						  unsigned long size)<br>
-{<br>
-	if (icache_is_aliasing()) {<br>
-		/* any kind of VIPT cache */<br>
-		__flush_icache_all();<br>
-	} else if (is_kernel_in_hyp_mode() || !icache_is_vpipt()) {<br>
-		/* PIPT or VPIPT at EL2 (see comment in __kvm_tlb_flush_vmid_ipa) */<br>
-		void *va = page_address(pfn_to_page(pfn));<br>
-<br>
-		invalidate_icache_range((unsigned long)va,<br>
-					(unsigned long)va + size);<br>
-	}<br>
-}<br>
-<br>
  void kvm_set_way_flush(struct kvm_vcpu *vcpu);<br>
  void kvm_toggle_cache(struct kvm_vcpu *vcpu, bool was_enabled);<br>
  
diff --git a/arch/arm64/kvm/hyp/pgtable.c b/arch/arm64/kvm/hyp/pgtable.c
<br>
index 4d177ce1d536..829a34eea526 100644<br>
--- a/arch/arm64/kvm/hyp/pgtable.c<br>
+++ b/arch/arm64/kvm/hyp/pgtable.c<br>
@@ -464,6 +464,43 @@ static int stage2_map_set_prot_attr(enum kvm_pgtable_prot prot,<br>
  	return 0;<br>
  }<br>
  
+static bool stage2_pte_cacheable(kvm_pte_t pte)
<br>
+{<br>
+	u64 memattr = pte & KVM_PTE_LEAF_ATTR_LO_S2_MEMATTR;<br>
+	return memattr == PAGE_S2_MEMATTR(NORMAL);<br>
+}<br>
+<br>
+static bool stage2_pte_executable(kvm_pte_t pte)<br>
+{<br>
+	return !(pte & KVM_PTE_LEAF_ATTR_HI_S2_XN);<br>
+}<br>
+<br>
+static void stage2_flush_dcache(void *addr, u64 size)<br>
+{<br>
+	/*<br>
+	 * With FWB, we ensure that the guest always accesses memory using<br>
+	 * cacheable attributes, and we don't have to clean to PoC when<br>
+	 * faulting in pages. Furthermore, FWB implies IDC, so cleaning to<br>
+	 * PoU is not required either in this case.<br>
+	 */<br>
+	if (cpus_have_const_cap(ARM64_HAS_STAGE2_FWB))<br>
+		return;<br>
+<br>
+	__flush_dcache_area(addr, size);<br>
+}<br>
+<br>
+static void stage2_invalidate_icache(void *addr, u64 size)<br>
+{<br>
+	if (icache_is_aliasing()) {<br>
+		/* Flush any kind of VIPT icache */<br>
+		__flush_icache_all();<br>
+	} else if (is_kernel_in_hyp_mode() || !icache_is_vpipt()) {<br>
+		/* PIPT or VPIPT at EL2 */<br>
+		invalidate_icache_range((unsigned long)addr,<br>
+					(unsigned long)addr + size);<br>
+	}<br>
+}<br>
+<br>
  static int stage2_map_walker_try_leaf(u64 addr, u64 end, u32 level,<br>
  				      kvm_pte_t *ptep,<br>
  				      struct stage2_map_data *data)<br>
@@ -495,6 +532,13 @@ static int stage2_map_walker_try_leaf(u64 addr, u64 end, u32 level,<br>
  		put_page(page);<br>
  	}<br>
  
+	/* Perform CMOs before installation of the new PTE */
<br>
+	if (!kvm_pte_valid(old) || stage2_pte_cacheable(old))<br>
</blockquote>
I'm not sure why the stage2_pte_cacheable(old) condition is needed.<br>
<br>
kvm_handle_guest_abort() handles three types of stage 2 data or instruction<br>
aborts: translation faults (fault_status == FSC_FAULT), access faults<br>
(fault_status == FSC_ACCESS) and permission faults (fault_status == FSC_PERM).<br>
<br>
Access faults are handled in handle_access_fault(), which means user_mem_abort()<br>
handles translation and permission faults.<br>
</blockquote>Yes, and we are certain that it's a translation fault here in 
stage2_map_walker_try_leaf.
<br><blockquote style="border-left: #5555EE solid 0.2em; margin: 0em; padding-left: 0.85em">
The original code did the dcache clean<br>
+ inval when not a permission fault, which means the CMO was done only on a<br>
translation fault. Translation faults mean that the IPA was not mapped, so the old<br>
entry will always be invalid. Even if we're coalescing multiple last level leaf<br>
entries int oaÂ  block mapping, the table entry which is replaced is invalid<br>
because it's marked as such in stage2_map_walk_table_pre().<br>
<br>
Is there something I'm missing?<br>
</blockquote>I originally thought that we could possibly have a translation fault on 
a valid stage2 table
<br>descriptor due to some special cases, and that's the reason 
stage2_pte_cacheable(old)
<br>
condition exits, but I can't image any scenario like this.<br>
<br>
I think your above explanation is right, maybe I should just drop that 
condition.
<br><blockquote style="border-left: #5555EE solid 0.2em; margin: 0em; padding-left: 0.85em">
<br>
<blockquote style="border-left: #5555EE solid 0.2em; margin: 0em; padding-left: 0.85em">
+		stage2_flush_dcache(__va(phys), granule);<br>
+<br>
+	if (stage2_pte_executable(new))<br>
+		stage2_invalidate_icache(__va(phys), granule);<br>
</blockquote>
This, together with the stage2_attr_walker() changes below, look identical to the<br>
current code in user_mem_abort(). The executable permission is set on an exec<br>
fault (instruction abort not on a stage 2 translation table walk), and as a result<br>
of the fault we either need to map a new page here, or relax permissions in<br>
kvm_pgtable_stage2_relax_perms() -> stage2_attr_walker() below.<br>
</blockquote>
I agree.<br>
Do you mean this part of change is right?<br>
<br>
Thanks,<br>
Yanan<br>
<blockquote style="border-left: #5555EE solid 0.2em; margin: 0em; padding-left: 0.85em">
Thanks,<br>
<br>
Alex<br>
<br>
<blockquote style="border-left: #5555EE solid 0.2em; margin: 0em; padding-left: 0.85em">
+<br>
  	smp_store_release(ptep, new);<br>
  	get_page(page);<br>
  	data->phys += granule;<br>
@@ -651,20 +695,6 @@ int kvm_pgtable_stage2_map(struct kvm_pgtable *pgt, u64 addr, u64 size,<br>
  	return ret;<br>
  }<br>
  
-static void stage2_flush_dcache(void *addr, u64 size)
<br>
-{<br>
-	if (cpus_have_const_cap(ARM64_HAS_STAGE2_FWB))<br>
-		return;<br>
-<br>
-	__flush_dcache_area(addr, size);<br>
-}<br>
-<br>
-static bool stage2_pte_cacheable(kvm_pte_t pte)<br>
-{<br>
-	u64 memattr = pte & KVM_PTE_LEAF_ATTR_LO_S2_MEMATTR;<br>
-	return memattr == PAGE_S2_MEMATTR(NORMAL);<br>
-}<br>
-<br>
  static int stage2_unmap_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,<br>
  			       enum kvm_pgtable_walk_flags flag,<br>
  			       void * const arg)<br>
@@ -743,8 +773,16 @@ static int stage2_attr_walker(u64 addr, u64 end, u32 level, kvm_pte_t *ptep,<br>
  	 * but worst-case the access flag update gets lost and will be<br>
  	 * set on the next access instead.<br>
  	 */<br>
-	if (data->pte != pte)<br>
+	if (data->pte != pte) {<br>
+		/*<br>
+		 * Invalidate the instruction cache before updating<br>
+		 * if we are going to add the executable permission.<br>
+		 */<br>
+		if (!stage2_pte_executable(*ptep) && stage2_pte_executable(pte))<br>
+			stage2_invalidate_icache(kvm_pte_follow(pte),<br>
+						 kvm_granule_size(level));<br>
  		WRITE_ONCE(*ptep, pte);<br>
+	}<br>
  
  	return 0;
<br>
  }<br>
diff --git a/arch/arm64/kvm/mmu.c b/arch/arm64/kvm/mmu.c<br>
index 77cb2d28f2a4..1eec9f63bc6f 100644<br>
--- a/arch/arm64/kvm/mmu.c<br>
+++ b/arch/arm64/kvm/mmu.c<br>
@@ -609,16 +609,6 @@ void kvm_arch_mmu_enable_log_dirty_pt_masked(struct kvm *kvm,<br>
  	kvm_mmu_write_protect_pt_masked(kvm, slot, gfn_offset, mask);<br>
  }<br>
  
-static void clean_dcache_guest_page(kvm_pfn_t pfn, unsigned long size)
<br>
-{<br>
-	__clean_dcache_guest_page(pfn, size);<br>
-}<br>
-<br>
-static void invalidate_icache_guest_page(kvm_pfn_t pfn, unsigned long size)<br>
-{<br>
-	__invalidate_icache_guest_page(pfn, size);<br>
-}<br>
-<br>
  static void kvm_send_hwpoison_signal(unsigned long address, short lsb)<br>
  {<br>
  	send_sig_mceerr(BUS_MCEERR_AR, (void __user *)address, lsb, current);<br>
@@ -882,13 +872,8 @@ static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,<br>
  	if (writable)<br>
  		prot |= KVM_PGTABLE_PROT_W;<br>
  
-	if (fault_status != FSC_PERM && !device)
<br>
-		clean_dcache_guest_page(pfn, vma_pagesize);<br>
-<br>
-	if (exec_fault) {<br>
+	if (exec_fault)<br>
  		prot |= KVM_PGTABLE_PROT_X;<br>
-		invalidate_icache_guest_page(pfn, vma_pagesize);<br>
-	}<br>
  
  	if (device)
<br>
  		prot |= KVM_PGTABLE_PROT_DEVICE;<br>
@@ -1144,10 +1129,10 @@ int kvm_set_spte_hva(struct kvm *kvm, unsigned long hva, pte_t pte)<br>
  	trace_kvm_set_spte_hva(hva);<br>
  
  	/*
<br>
-	 * We've moved a page around, probably through CoW, so let's treat it<br>
-	 * just like a translation fault and clean the cache to the PoC.<br>
+	 * We've moved a page around, probably through CoW, so let's treat<br>
+	 * it just like a translation fault and the map handler will clean<br>
+	 * the cache to the PoC.<br>
  	 */<br>
-	clean_dcache_guest_page(pfn, PAGE_SIZE);<br>
  	handle_hva_to_gpa(kvm, hva, end, &kvm_set_spte_handler, &pfn);<br>
  	return 0;<br>
  }<br>
</blockquote>
.<br>
</blockquote>
<br>
<br>

