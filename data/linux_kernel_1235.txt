
<br>
<br>
On 08/04/2021 15:37, Michael Ellerman wrote:<br>
<blockquote style="border-left: #5555EE solid 0.2em; margin: 0em; padding-left: 0.85em">
Leonardo Bras <leobras.c@xxxxxxxxx> writes:<br>
<blockquote style="border-left: #5555EE solid 0.2em; margin: 0em; padding-left: 0.85em">
According to LoPAR, ibm,query-pe-dma-window output named "IO Page Sizes"<br>
will let the OS know all possible pagesizes that can be used for creating a<br>
new DDW.<br>
<br>
Currently Linux will only try using 3 of the 8 available options:<br>
4K, 64K and 16M. According to LoPAR, Hypervisor may also offer 32M, 64M,<br>
128M, 256M and 16G.<br>
</blockquote>
<br>
Do we know of any hardware & hypervisor combination that will actually<br>
give us bigger pages?<br>
</blockquote>
<br>
<br>
On P8 16MB host pages and 16MB hardware iommu pages worked.<br>
<br>
On P9, VM's 16MB IOMMU pages worked on top of 2MB host pages + 2MB 
hardware IOMMU pages.
<br>
<br>
<br>
<blockquote style="border-left: #5555EE solid 0.2em; margin: 0em; padding-left: 0.85em">
<br>
<blockquote style="border-left: #5555EE solid 0.2em; margin: 0em; padding-left: 0.85em">
Enabling bigger pages would be interesting for direct mapping systems<br>
with a lot of RAM, while using less TCE entries.<br>
<br>
Signed-off-by: Leonardo Bras <leobras.c@xxxxxxxxx><br>
---<br>
  arch/powerpc/platforms/pseries/iommu.c | 49 ++++++++++++++++++++++----<br>
  1 file changed, 42 insertions(+), 7 deletions(-)<br>
<br>
diff --git a/arch/powerpc/platforms/pseries/iommu.c b/arch/powerpc/platforms/pseries/iommu.c<br>
index 9fc5217f0c8e..6cda1c92597d 100644<br>
--- a/arch/powerpc/platforms/pseries/iommu.c<br>
+++ b/arch/powerpc/platforms/pseries/iommu.c<br>
@@ -53,6 +53,20 @@ enum {<br>
  	DDW_EXT_QUERY_OUT_SIZE = 2<br>
  };<br>
</blockquote>
<br>
A comment saying where the values come from would be good.<br>
<br>
<blockquote style="border-left: #5555EE solid 0.2em; margin: 0em; padding-left: 0.85em">
+#define QUERY_DDW_PGSIZE_4K	0x01<br>
+#define QUERY_DDW_PGSIZE_64K	0x02<br>
+#define QUERY_DDW_PGSIZE_16M	0x04<br>
+#define QUERY_DDW_PGSIZE_32M	0x08<br>
+#define QUERY_DDW_PGSIZE_64M	0x10<br>
+#define QUERY_DDW_PGSIZE_128M	0x20<br>
+#define QUERY_DDW_PGSIZE_256M	0x40<br>
+#define QUERY_DDW_PGSIZE_16G	0x80<br>
</blockquote>
<br>
I'm not sure the #defines really gain us much vs just putting the<br>
literal values in the array below?<br>
</blockquote>
<br>
<br>
Then someone says "uuuuu magic values" :) I do not mind either way. Thanks,<br>
<br>
<br>
<br>
<blockquote style="border-left: #5555EE solid 0.2em; margin: 0em; padding-left: 0.85em"><blockquote style="border-left: #5555EE solid 0.2em; margin: 0em; padding-left: 0.85em">
+struct iommu_ddw_pagesize {<br>
+	u32 mask;<br>
+	int shift;<br>
+};<br>
+<br>
  static struct iommu_table_group *iommu_pseries_alloc_group(int node)<br>
  {<br>
  	struct iommu_table_group *table_group;<br>
@@ -1099,6 +1113,31 @@ static void reset_dma_window(struct pci_dev *dev, struct device_node *par_dn)<br>
  			 ret);<br>
  }<br>
  
+/* Returns page shift based on "IO Page Sizes" output at ibm,query-pe-dma-window. See LoPAR */
<br>
+static int iommu_get_page_shift(u32 query_page_size)<br>
+{<br>
+	const struct iommu_ddw_pagesize ddw_pagesize[] = {<br>
+		{ QUERY_DDW_PGSIZE_16G,  __builtin_ctz(SZ_16G)  },<br>
+		{ QUERY_DDW_PGSIZE_256M, __builtin_ctz(SZ_256M) },<br>
+		{ QUERY_DDW_PGSIZE_128M, __builtin_ctz(SZ_128M) },<br>
+		{ QUERY_DDW_PGSIZE_64M,  __builtin_ctz(SZ_64M)  },<br>
+		{ QUERY_DDW_PGSIZE_32M,  __builtin_ctz(SZ_32M)  },<br>
+		{ QUERY_DDW_PGSIZE_16M,  __builtin_ctz(SZ_16M)  },<br>
+		{ QUERY_DDW_PGSIZE_64K,  __builtin_ctz(SZ_64K)  },<br>
+		{ QUERY_DDW_PGSIZE_4K,   __builtin_ctz(SZ_4K)   }<br>
+	};<br>
</blockquote>
<br>
<br>
cheers<br>
<br>
</blockquote>
<br>
--<br>
Alexey<br>
<br>
<br>

