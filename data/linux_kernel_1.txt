This patch provides a vruntime based way to compare two cfs task's<br>
priority, be it on the same cpu or different threads of the same core.<br>
<br>
When the two tasks are on the same CPU, we just need to find a common<br>
cfs_rq both sched_entities are on and then do the comparison.<br>
<br>
When the two tasks are on differen threads of the same core, each thread<br>
will choose the next task to run the usual way and then the root level<br>
sched entities which the two tasks belong to will be used to decide<br>
which task runs next core wide.<br>
<br>
An illustration for the cross CPU case:<br>
<br>
   cpu0         cpu1<br>
 /   |  \     /   |  \<br>
se1 se2 se3  se4 se5 se6<br>
    /  \            /   \<br>
  se21 se22       se61  se62<br>
  (A)                    /<br>
                       se621<br>
                        (B)<br>
<br>
Assume CPU0 and CPU1 are smt siblings and cpu0 has decided task A to<br>
run next and cpu1 has decided task B to run next. To compare priority<br>
of task A and B, we compare priority of se2 and se6. Whose vruntime is<br>
smaller, who wins.<br>
<br>
To make this work, the root level sched entities' vruntime of the two<br>
threads must be directly comparable. So a new core wide cfs_rq<br>
min_vruntime is introduced to serve the purpose of normalizing these<br>
root level sched entities' vruntime.<br>
<br>
All sub cfs_rqs and sched entities are not interesting in cross cpu<br>
priority comparison as they will only participate in the usual cpu local<br>
schedule decision so no need to normalize their vruntimes.<br>
<br>
Signed-off-by: Aaron Lu <ziqian.lzq@xxxxxxxxxx><br>
---<br>
 kernel/sched/core.c  |  24 ++++------<br>
 kernel/sched/fair.c  | 101 ++++++++++++++++++++++++++++++++++++++++++-<br>
 kernel/sched/sched.h |   3 ++<br>
 3 files changed, 111 insertions(+), 17 deletions(-)<br>
<br>
diff --git a/kernel/sched/core.c b/kernel/sched/core.c<br>
index 5f322922f5ae..d6c8c76cb07a 100644<br>
--- a/kernel/sched/core.c<br>
+++ b/kernel/sched/core.c<br>
@@ -119,19 +119,8 @@ static inline bool prio_less(struct task_struct *a, struct task_struct *b)<br>
 	if (pa == -1) /* dl_prio() doesn't work because of stop_class above */<br>
 		return !dl_time_before(a->dl.deadline, b->dl.deadline);<br>
 <br>
-	if (pa == MAX_RT_PRIO + MAX_NICE)  { /* fair */<br>
-		u64 vruntime = b->se.vruntime;<br>
-<br>
-		/*<br>
-		 * Normalize the vruntime if tasks are in different cpus.<br>
-		 */<br>
-		if (task_cpu(a) != task_cpu(b)) {<br>
-			vruntime -= task_cfs_rq(b)->min_vruntime;<br>
-			vruntime += task_cfs_rq(a)->min_vruntime;<br>
-		}<br>
-<br>
-		return !((s64)(a->se.vruntime - vruntime) <= 0);<br>
-	}<br>
+	if (pa == MAX_RT_PRIO + MAX_NICE) /* fair */<br>
+		return cfs_prio_less(a, b);<br>
 <br>
 	return false;<br>
 }<br>
@@ -291,8 +280,13 @@ static int __sched_core_stopper(void *data)<br>
 	}<br>
 <br>
 	for_each_online_cpu(cpu) {<br>
-		if (!enabled || (enabled && cpumask_weight(cpu_smt_mask(cpu)) >= 2))<br>
-			cpu_rq(cpu)->core_enabled = enabled;<br>
+		if (!enabled || (enabled && cpumask_weight(cpu_smt_mask(cpu)) >= 2)) {<br>
+			struct rq *rq = cpu_rq(cpu);<br>
+<br>
+			rq->core_enabled = enabled;<br>
+			if (rq->core == rq)<br>
+				sched_core_adjust_se_vruntime(cpu);<br>
+		}<br>
 	}<br>
 <br>
 	return 0;<br>
diff --git a/kernel/sched/fair.c b/kernel/sched/fair.c<br>
index d99ea6ee7af2..7eecf590d6c0 100644<br>
--- a/kernel/sched/fair.c<br>
+++ b/kernel/sched/fair.c<br>
@@ -449,9 +449,103 @@ find_matching_se(struct sched_entity **se, struct sched_entity **pse)<br>
 <br>
 #endif	/* CONFIG_FAIR_GROUP_SCHED */<br>
 <br>
+static inline struct cfs_rq *root_cfs_rq(struct cfs_rq *cfs_rq)<br>
+{<br>
+	return &rq_of(cfs_rq)->cfs;<br>
+}<br>
+<br>
+static inline bool is_root_cfs_rq(struct cfs_rq *cfs_rq)<br>
+{<br>
+	return cfs_rq == root_cfs_rq(cfs_rq);<br>
+}<br>
+<br>
+static inline struct cfs_rq *core_cfs_rq(struct cfs_rq *cfs_rq)<br>
+{<br>
+	return &rq_of(cfs_rq)->core->cfs;<br>
+}<br>
+<br>
 static inline u64 cfs_rq_min_vruntime(struct cfs_rq *cfs_rq)<br>
 {<br>
-	return cfs_rq->min_vruntime;<br>
+	if (!sched_core_enabled(rq_of(cfs_rq)) || !is_root_cfs_rq(cfs_rq))<br>
+		return cfs_rq->min_vruntime;<br>
+<br>
+	return core_cfs_rq(cfs_rq)->min_vruntime;<br>
+}<br>
+<br>
+bool cfs_prio_less(struct task_struct *a, struct task_struct *b)<br>
+{<br>
+	struct sched_entity *sea = &a->se;<br>
+	struct sched_entity *seb = &b->se;<br>
+	bool samecpu = task_cpu(a) == task_cpu(b);<br>
+	s64 delta;<br>
+<br>
+	if (samecpu) {<br>
+		/* vruntime is per cfs_rq */<br>
+		while (!is_same_group(sea, seb)) {<br>
+			int sea_depth = sea->depth;<br>
+			int seb_depth = seb->depth;<br>
+<br>
+			if (sea_depth >= seb_depth)<br>
+				sea = parent_entity(sea);<br>
+			if (sea_depth <= seb_depth)<br>
+				seb = parent_entity(seb);<br>
+		}<br>
+<br>
+		delta = (s64)(sea->vruntime - seb->vruntime);<br>
+		goto out;<br>
+	}<br>
+<br>
+	/* crosscpu: compare root level se's vruntime to decide priority */<br>
+	while (sea->parent)<br>
+		sea = sea->parent;<br>
+	while (seb->parent)<br>
+		seb = seb->parent;<br>
+	delta = (s64)(sea->vruntime - seb->vruntime);<br>
+<br>
+out:<br>
+	return delta > 0;<br>
+}<br>
+<br>
+/*<br>
+ * This is called in stop machine context so no need to take the rq lock.<br>
+ *<br>
+ * Core scheduling is going to be enabled and the root level sched entities<br>
+ * of both siblings will use cfs_rq->min_vruntime as the common cfs_rq<br>
+ * min_vruntime, so it's necessary to normalize vruntime of existing root<br>
+ * level sched entities in sibling_cfs_rq.<br>
+ *<br>
+ * Update of sibling_cfs_rq's min_vruntime isn't necessary as we will be<br>
+ * only using cfs_rq->min_vruntime during the entire run of core scheduling.<br>
+ */<br>
+void sched_core_adjust_se_vruntime(int cpu)<br>
+{<br>
+	int i;<br>
+<br>
+	for_each_cpu(i, cpu_smt_mask(cpu)) {<br>
+		struct cfs_rq *cfs_rq, *sibling_cfs_rq;<br>
+		struct sched_entity *se, *next;<br>
+		s64 delta;<br>
+<br>
+		if (i == cpu)<br>
+			continue;<br>
+<br>
+		sibling_cfs_rq = &cpu_rq(i)->cfs;<br>
+		if (!sibling_cfs_rq->nr_running)<br>
+			continue;<br>
+<br>
+		cfs_rq = &cpu_rq(cpu)->cfs;<br>
+		delta = cfs_rq->min_vruntime - sibling_cfs_rq->min_vruntime;<br>
+		/*<br>
+		 * XXX Malicious user can create a ton of runnable tasks in root<br>
+		 * sibling_cfs_rq and cause the below vruntime normalization<br>
+		 * potentially taking a long time.<br>
+		 */<br>
+		rbtree_postorder_for_each_entry_safe(se, next,<br>
+				&sibling_cfs_rq->tasks_timeline.rb_root,<br>
+				run_node) {<br>
+			se->vruntime += delta;<br>
+		}<br>
+	}<br>
 }<br>
 <br>
 static __always_inline<br>
@@ -509,8 +603,11 @@ static void update_min_vruntime(struct cfs_rq *cfs_rq)<br>
 			vruntime = min_vruntime(vruntime, se->vruntime);<br>
 	}<br>
 <br>
+	if (sched_core_enabled(rq_of(cfs_rq)) && is_root_cfs_rq(cfs_rq))<br>
+		cfs_rq = core_cfs_rq(cfs_rq);<br>
+<br>
 	/* ensure we never gain time by being placed backwards. */<br>
-	cfs_rq->min_vruntime = max_vruntime(cfs_rq_min_vruntime(cfs_rq), vruntime);<br>
+	cfs_rq->min_vruntime = max_vruntime(cfs_rq->min_vruntime, vruntime);<br>
 #ifndef CONFIG_64BIT<br>
 	smp_wmb();<br>
 	cfs_rq->min_vruntime_copy = cfs_rq->min_vruntime;<br>
diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h<br>
index 50a5675e941a..24bae760f764 100644<br>
--- a/kernel/sched/sched.h<br>
+++ b/kernel/sched/sched.h<br>
@@ -2594,3 +2594,6 @@ static inline void membarrier_switch_mm(struct rq *rq,<br>
 {<br>
 }<br>
 #endif<br>
+<br>
+bool cfs_prio_less(struct task_struct *a, struct task_struct *b);<br>
+void sched_core_adjust_se_vruntime(int cpu);<br>
-- <br>
2.19.1.3.ge56e4f7<br>
<br>
<br>

